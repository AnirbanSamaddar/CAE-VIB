{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7977c78d",
   "metadata": {},
   "source": [
    "# Calculating the integrated gradients from the CAE-VIB model:\n",
    "\n",
    "**Importing necessary packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b96f412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python import tf2\n",
    "if not tf2.enabled():\n",
    "  import tensorflow.compat.v2 as tf\n",
    "  tf.enable_v2_behavior() \n",
    "  assert tf2.enabled()\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob \n",
    "\n",
    "mode = 'train'   \n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))\n",
    "\n",
    "import time\n",
    "time1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecde1e8",
   "metadata": {},
   "source": [
    "**Defining data loading functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c607a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 9\n",
    "num_tot = 100\n",
    "num_epochs = 200\n",
    "nGrid= 128 \n",
    "max_evals = 5 \n",
    "dtype = 'non-rotated'\n",
    "\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "def shuffle(X, y):\n",
    "    shuffleOrder = np.arange(X.shape[0])\n",
    "    np.random.shuffle(shuffleOrder)\n",
    "    X = X[shuffleOrder]\n",
    "    y = y[shuffleOrder]\n",
    "    return X, y, shuffleOrder\n",
    "\n",
    "def load_data_TrainTestVal(n):\n",
    "    os.chdir('./Results_2/')\n",
    "    dirIn0 = '../data/Data_split-' + dtype + '/Data_split/'\n",
    "    dirIn1 = 'Data_Split_N'+str(n)+'/'\n",
    "    dirIn = dirIn0 + dirIn1\n",
    "    print(dirIn)\n",
    "    print(glob.glob(dirIn + 'input_train_data*'))\n",
    "    train_data = np.load(glob.glob(dirIn + 'input_train_data*')[0])\n",
    "    train_target = np.load(glob.glob(dirIn + 'output_train_data*')[0])\n",
    "    test_data = np.load(glob.glob(dirIn + 'input_test_data*')[0])\n",
    "    test_target = np.load(glob.glob(dirIn + 'output_test_data*')[0])\n",
    "    valid_data = np.load(glob.glob(dirIn + 'input_val_data*')[0])\n",
    "    valid_target = np.load(glob.glob(dirIn + 'output_val_data*')[0]) \n",
    "    return (train_data, train_target), (test_data, test_target), (valid_data, valid_target)\n",
    "\n",
    "\n",
    "def load_data_prepared():\n",
    "    (train_input, train_target), (test_input, test_target), (valid_input, valid_target) = load_data_TrainTestVal(num_tot)\n",
    "    tmin = train_input.min()    \n",
    "    tmax = train_input.max()\n",
    "    train_images = (train_input - tmin) / (tmax - tmin) # Normalize the images to [-1, 1]\n",
    "    test_images = (test_input - tmin) / (tmax - tmin)\n",
    "    valid_images = (valid_input - tmin) / (tmax - tmin)\n",
    "    shape0 = train_images.shape[0]\n",
    "    swe_train_data = train_images.reshape(shape0, nGrid, nGrid, 1)\n",
    "    shape0 = test_images.shape[0]\n",
    "    swe_test_data = test_images.reshape(shape0, nGrid, nGrid, 1)   \n",
    "    shape0 = valid_images.shape[0]\n",
    "    swe_valid_data = valid_images.reshape(shape0, nGrid, nGrid, 1)        \n",
    "    ############################################    \n",
    "    tmin = train_target.min()    \n",
    "    tmax = train_target.max()\n",
    "    train_target = (train_target - tmin) / (tmax - tmin) # Normalize the images to [-1, 1]\n",
    "    test_target = (test_target - tmin) / (tmax - tmin)\n",
    "    valid_target = (valid_target - tmin) / (tmax - tmin)\n",
    "    shape0 = train_target.shape[0]\n",
    "    swe_train_target = train_target.reshape(shape0, nGrid, nGrid, 1)\n",
    "    shape0 = test_target.shape[0]\n",
    "    swe_test_target = test_target.reshape(shape0, nGrid, nGrid, 1)    \n",
    "    shape0 = valid_target.shape[0]\n",
    "    swe_valid_target = valid_target.reshape(shape0, nGrid, nGrid, 1)    \n",
    "    return (swe_train_data, swe_train_target), (swe_test_data, swe_test_target), (swe_valid_data, swe_valid_target), (tmax, tmin)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f25d1",
   "metadata": {},
   "source": [
    "**Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "669e68a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Data_split-non-rotated/Data_split/Data_Split_N100/\n",
      "['../data/Data_split-non-rotated/Data_split/Data_Split_N100/input_train_data_90.npy']\n"
     ]
    }
   ],
   "source": [
    "(swe_train_data, swe_train_target), (swe_test_data, swe_test_target), (swe_valid_data, swe_valid_target), (tmax, tmin) = load_data_prepared()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e504b4",
   "metadata": {},
   "source": [
    "**Defining necessary functions for the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbfbce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unscale(field):\n",
    "    mx = (tmax - tmin)\n",
    "    unscaled = (field*mx) + tmin \n",
    "    return unscaled\n",
    "\n",
    "def sampling(z):\n",
    "    return z[0] + z[1] * tf.random.normal(shape=z[0].shape, mean=0.0, stddev=1.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f7f62",
   "metadata": {},
   "source": [
    "**Defining the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1501a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model(config: dict, verbose: bool = 0,seed:int = 1):\n",
    "    default_config = {\n",
    "        \"dense_width\": 128,\n",
    "        \"decay_rate\": 0.999,\n",
    "        \"decay_rate_steps\": 1,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 8,\n",
    "        \"latent_dim\": 32,\n",
    "        \"beta\":10**(-2)\n",
    "    }\n",
    "    default_config.update(config)\n",
    "    print(default_config)\n",
    "    default_config[\"dense_width\"] = int(default_config[\"dense_width\"])\n",
    "    default_config[\"latent_dim\"] = int(default_config[\"latent_dim\"])\n",
    "    default_config[\"batch_size\"] = int(default_config[\"batch_size\"])\n",
    "    ############### LOADING #############################################\n",
    "    (swe_train_data, swe_train_target), (swe_test_data, swe_test_target), (swe_valid_data, swe_valid_target), (tmax, tmin) = load_data_prepared()\n",
    "    ###### Encoder ###################\n",
    "    encoder_inputs = Input(shape=(nGrid, nGrid, 1), name='Field')\n",
    "    x= Conv2D(128, kernel_size=3, activation='relu', padding='same'\n",
    "              ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(encoder_inputs)\n",
    "    x= Conv2D(128, kernel_size=3, strides=1, activation='relu', padding='same'\n",
    "              ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)\n",
    "    x= Conv2D(128, kernel_size=3, strides=1, activation='relu', padding='same'\n",
    "             ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)   \n",
    "    x= Conv2D(128, kernel_size=3, strides=1, activation='relu', padding='same'\n",
    "             ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)   \n",
    "    x= Conv2D(128, kernel_size=3, strides=1, activation='relu', padding='same'\n",
    "             ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)   \n",
    "    x= Conv2D(128, kernel_size=3, strides=1, activation='relu', padding='same'\n",
    "             ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)   \n",
    "    x= Conv2D(128, kernel_size=3, strides=1, activation='relu', padding='same'\n",
    "             ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)   \n",
    "    x= Conv2D(128, kernel_size=3, strides=2, activation='relu', padding='same'\n",
    "             ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)   \n",
    "    x= Conv2D(64, kernel_size=3, strides=2, activation='relu', padding='same'\n",
    "             ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)   \n",
    "    x= Conv2D(64, kernel_size=3, strides=2, activation='relu', padding='same'\n",
    "             ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)   \n",
    "    x= Flatten()(x)\n",
    "    x= Dense(256,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)   \n",
    "    x= Dense(default_config[\"dense_width\"],kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)   \n",
    "    z_mean_sigma= Dense(2*default_config[\"latent_dim\"], name='z_mean_sigma'\n",
    "                       ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)\n",
    "\n",
    "    encoder = Model(inputs=encoder_inputs, outputs=z_mean_sigma, name='encoder')\n",
    "    ###### Decoder ################\n",
    "    latent_inputs = Input(shape=(default_config[\"latent_dim\"],), name='z_sampling')\n",
    "    dim1 = 128\n",
    "    x= Dense(default_config[\"dense_width\"],kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(latent_inputs)\n",
    "    x= Dense(256, activation='relu',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)\n",
    "    x= Dense(dim1*dim1, activation='relu',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)\n",
    "    x= Reshape((dim1, dim1, 1))(x)\n",
    "    x = Conv2DTranspose(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'\n",
    "                       ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)\n",
    "    x = Conv2DTranspose(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'\n",
    "                       ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)\n",
    "    x = Conv2DTranspose(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'\n",
    "                       ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)\n",
    "    x = Conv2DTranspose(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'\n",
    "                       ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)\n",
    "    x = Conv2DTranspose(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'\n",
    "                       ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)\n",
    "    x = Conv2DTranspose(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'\n",
    "                       ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)\n",
    "    decoded = Conv2DTranspose(1, kernel_size=3, activation=None, padding='same'\n",
    "                             ,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed))(x)\n",
    "    decoder = Model(inputs=latent_inputs, outputs=decoded)\n",
    "    ##########################################\n",
    "    z_mean_sigma_out = encoder(encoder_inputs)\n",
    "    print(z_mean_sigma_out.shape)\n",
    "    mean_t = z_mean_sigma_out[:,:default_config[\"latent_dim\"]]\n",
    "    sigma_t = tf.keras.activations.softplus(z_mean_sigma_out[:,default_config[\"latent_dim\"]:])\n",
    "    print(mean_t.shape)\n",
    "    print(sigma_t.shape)\n",
    "    z_mean = mean_t + sigma_t * tf.random.normal(shape=(default_config[\"latent_dim\"],), mean=0.0, stddev=1.0)\n",
    "    ae_outputs = decoder(z_mean)\n",
    "    ##########################################    \n",
    "    def CustomLoss(y_true, y_pred, input_tensor,mean_t,sigma_t):\n",
    "        conv_inp_out = y_pred*input_tensor\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        result = -0.5 * (1 + 2 * tf.math.log(sigma_t) - tf.math.pow(mean_t, 2) - tf.math.pow(sigma_t, 2))\n",
    "        result_sum = tf.reduce_sum(result, axis=1)\n",
    "        result_mean = tf.reduce_mean(result_sum)\n",
    "        kl = result_mean / math.log(2)\n",
    "        ITY = -0.5 * tf.math.log(mse(conv_inp_out, y_true))/tf.math.log(2.)\n",
    "        return -ITY+ default_config[\"beta\"]*kl  \n",
    "    target = Input(shape=(nGrid, nGrid, 1), name='Field_target')\n",
    "    model = Model(inputs=[encoder_inputs, target], outputs=ae_outputs, name='AE')    \n",
    "    model.add_loss(CustomLoss( target, ae_outputs, encoder_inputs, mean_t,sigma_t ) )\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=default_config[\"learning_rate\"],\n",
    "                    decay_steps=default_config[\"decay_rate_steps\"],\n",
    "                    decay_rate=default_config[\"decay_rate\"],staircase=True)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=opt)\n",
    "    if mode == 'train':\n",
    "        earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "        callbacks_list = (earlystopping)\n",
    "        train_history = model.fit(x=[swe_train_data, swe_train_target], \n",
    "                                  y=None, \n",
    "                                  epochs=num_epochs, \n",
    "                                  batch_size=default_config[\"batch_size\"], \n",
    "                                  callbacks=callbacks_list,\n",
    "                                  verbose=verbose,\n",
    "                                  validation_data=([swe_valid_data, swe_valid_target], None)).history    \n",
    "    model = Model(model.input[0], model.output)\n",
    "    #########################     \n",
    "    swe_valid_pred = model.predict(swe_valid_data)[:, :, :, 0]    \n",
    "    x = swe_valid_data[:, :, :, :]\n",
    "    y_true = swe_valid_target[:, :, :, :]\n",
    "    y_pred = swe_valid_pred[:, :, :, np.newaxis]*swe_valid_data[:, :, :, :]    \n",
    "    metric_final = ((y_true - y_pred)**2).mean()\n",
    "    return metric_final, train_history, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0885734",
   "metadata": {},
   "source": [
    "**Defining the best configuration from 100 DeepHyper evaluations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0355b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {'batch_size': 16.0, 'beta': 0.001, 'decay_rate': 0.6157086900579446, 'decay_rate_steps': 71.0, 'dense_width': 256.0, 'latent_dim': 256.0, 'learning_rate': 0.0003380824712275}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332837ca",
   "metadata": {},
   "source": [
    "**Rerunning the best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e992ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dense_width': 256.0, 'decay_rate': 0.6157086900579446, 'decay_rate_steps': 71.0, 'learning_rate': 0.0003380824712275, 'batch_size': 16.0, 'latent_dim': 256.0, 'beta': 0.001}\n",
      "../data/Data_split-non-rotated/Data_split/Data_Split_N100/\n",
      "['../data/Data_split-non-rotated/Data_split/Data_Split_N100/input_train_data_90.npy']\n",
      "(None, 512)\n",
      "(None, 256)\n",
      "(None, 256)\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 6s 164ms/step - loss: -1.3585 - val_loss: -1.6844\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -2.9822 - val_loss: -3.6387\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -3.8294 - val_loss: -4.4224\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -4.2882 - val_loss: -4.2997\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -4.4644 - val_loss: -4.5494\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -4.6669 - val_loss: -4.5096\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -4.6675 - val_loss: -4.9670\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -4.9063 - val_loss: -4.5441\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 58ms/step - loss: -4.6717 - val_loss: -4.6821\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -4.7292 - val_loss: -5.0273\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -4.7589 - val_loss: -5.0153\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.0210 - val_loss: -5.1646\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.1931 - val_loss: -5.1554\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.1697 - val_loss: -5.1937\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2006 - val_loss: -5.1781\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.1916 - val_loss: -5.1803\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: -5.1972 - val_loss: -5.1503\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2193 - val_loss: -5.1860\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2057 - val_loss: -5.1775\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.1804 - val_loss: -5.2167\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2289 - val_loss: -5.1881\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2228 - val_loss: -5.1929\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2236 - val_loss: -5.2047\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2562 - val_loss: -5.2073\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2355 - val_loss: -5.2027\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2452 - val_loss: -5.2182\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2729 - val_loss: -5.2157\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2238 - val_loss: -5.2358\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2482 - val_loss: -5.2141\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2387 - val_loss: -5.2265\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2502 - val_loss: -5.2406\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2561 - val_loss: -5.2133\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2485 - val_loss: -5.2333\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2597 - val_loss: -5.2560\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2550 - val_loss: -5.1946\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2648 - val_loss: -5.2602\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2817 - val_loss: -5.2597\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.2751 - val_loss: -5.2755\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.3120 - val_loss: -5.2900\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.3393 - val_loss: -5.2859\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.3804 - val_loss: -5.4150\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.5481 - val_loss: -5.6540\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.6975 - val_loss: -5.8027\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.7980 - val_loss: -5.8898\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.9358 - val_loss: -5.5559\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.7028 - val_loss: -5.6440\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -5.7601 - val_loss: -6.0256\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.0728 - val_loss: -6.1733\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.2238 - val_loss: -6.2342\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.2734 - val_loss: -6.3369\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.3247 - val_loss: -6.4078\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.3913 - val_loss: -6.3656\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.4393 - val_loss: -6.3610\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.4410 - val_loss: -6.3981\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.4934 - val_loss: -6.6150\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.4394 - val_loss: -6.5951\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.2831 - val_loss: -6.3373\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 60ms/step - loss: -6.3588 - val_loss: -5.9197\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.0687 - val_loss: -5.9802\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.3386 - val_loss: -6.3231\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.5133 - val_loss: -6.5416\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.6392 - val_loss: -6.6303\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.6484 - val_loss: -6.4895\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.5988 - val_loss: -6.5526\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.5939 - val_loss: -6.6290\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.6764 - val_loss: -6.6851\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.7148 - val_loss: -6.6380\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.7417 - val_loss: -6.5900\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.6883 - val_loss: -6.5565\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 56ms/step - loss: -6.7623 - val_loss: -6.6824\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.7931 - val_loss: -6.6463\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8089 - val_loss: -6.6914\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8130 - val_loss: -6.6211\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.7985 - val_loss: -6.6721\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8215 - val_loss: -6.6670\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.7904 - val_loss: -6.6595\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8417 - val_loss: -6.6761\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8145 - val_loss: -6.6442\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8387 - val_loss: -6.6432\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8362 - val_loss: -6.6476\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8243 - val_loss: -6.6546\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8673 - val_loss: -6.6126\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 56ms/step - loss: -6.8097 - val_loss: -6.6686\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8096 - val_loss: -6.6665\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8750 - val_loss: -6.6590\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 58ms/step - loss: -6.8611 - val_loss: -6.6460\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8470 - val_loss: -6.6612\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8581 - val_loss: -6.6036\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8109 - val_loss: -6.6611\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8185 - val_loss: -6.5625\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8361 - val_loss: -6.6529\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9058 - val_loss: -6.6739\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8827 - val_loss: -6.6499\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8848 - val_loss: -6.6578\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9006 - val_loss: -6.6418\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8770 - val_loss: -6.6399\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 56ms/step - loss: -6.9474 - val_loss: -6.6433\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.8924 - val_loss: -6.6389\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9098 - val_loss: -6.6501\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9087 - val_loss: -6.6338\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9656 - val_loss: -6.6445\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9464 - val_loss: -6.6494\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9119 - val_loss: -6.6239\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9058 - val_loss: -6.6102\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9531 - val_loss: -6.6588\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9108 - val_loss: -6.6449\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9164 - val_loss: -6.6336\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9067 - val_loss: -6.6556\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9082 - val_loss: -6.6590\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9513 - val_loss: -6.6648\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9616 - val_loss: -6.6456\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 58ms/step - loss: -6.9365 - val_loss: -6.6496\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9441 - val_loss: -6.6621\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9501 - val_loss: -6.6613\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9188 - val_loss: -6.6575\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9447 - val_loss: -6.6551\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: -6.9552 - val_loss: -6.6481\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 63ms/step - loss: -6.9521 - val_loss: -6.6554\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9402 - val_loss: -6.6608\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9759 - val_loss: -6.6603\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9547 - val_loss: -6.6608\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: -6.9711 - val_loss: -6.6580\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "9.402872e-05\n"
     ]
    }
   ],
   "source": [
    "seed_value=9\n",
    "tf.keras.utils.set_random_seed(10+seed_value)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "metric_best, history_best, model_best  = build_and_train_model(\n",
    "    config=best_config, verbose=1,seed=seed_value)\n",
    "print(metric_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb793aa",
   "metadata": {},
   "source": [
    "**Plotting the train and validation loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e64a4844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if mode == 'train':\n",
    "    plt.figure(2111)\n",
    "    plt.plot(history_best['loss'], label = 'loss')\n",
    "    plt.plot(history_best['val_loss'], label = 'val loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e38e53",
   "metadata": {},
   "source": [
    "**Generating images from baseline to the original image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e24ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_images(baseline,\n",
    "                       image,\n",
    "                       alphas):\n",
    "  alphas_x = alphas[:, tf.newaxis, tf.newaxis, tf.newaxis]\n",
    "  baseline_x = tf.expand_dims(baseline, axis=0)\n",
    "  input_x = tf.expand_dims(image, axis=0)\n",
    "  delta = input_x - baseline_x\n",
    "  images = baseline_x +  alphas_x * delta\n",
    "  return images\n",
    "\n",
    "baseline = tf.zeros(shape=(128,128,1))\n",
    "m_steps=10\n",
    "alphas = tf.linspace(start=0.0, stop=1.0, num=m_steps+1)\n",
    "\n",
    "interpolated_images = interpolate_images(\n",
    "    baseline=baseline,\n",
    "    image=swe_test_data[0,],\n",
    "    alphas=alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79870b62",
   "metadata": {},
   "source": [
    "**Plotting the interpolated images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37b2abe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 11 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir('./Results_2/')\n",
    "\n",
    "dirOut0 = '../output/python/deephyper/'+dtype+'/'\n",
    "dirOut1 = 'plots_ae_'+dtype+'/N'+str(num_tot)+'/' \n",
    "Current_Model_directory = dirOut0 + dirOut1\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "i = 0\n",
    "for alpha, image in zip(alphas[0:len(alphas)], interpolated_images[0:len(alphas)]):\n",
    "  i += 1\n",
    "  plt.subplot(1, len(alphas[0:len(alphas)]), i)\n",
    "  if alpha != 0. and alpha != 1.:\n",
    "      plt.title(r'$\\alpha$ : '+f'{alpha:.1f}')\n",
    "  elif alpha == 0.:\n",
    "      plt.title('Baseline')\n",
    "  else:\n",
    "      plt.title('Original')\n",
    "  plt.imshow(image,vmin=0,vmax=1)\n",
    "  plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(Current_Model_directory+'interpolated_inputs_60.png',format = 'png',dpi=500,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a605061",
   "metadata": {},
   "source": [
    "**Defining functions for the integrated gradient calculation**\n",
    "\n",
    "Some of the functions below are taken from [this](https://www.tensorflow.org/tutorials/interpretability/integrated_gradients) tensorflow tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d029fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_encoder = Model(model_best.input, model_best.layers[-2].output)\n",
    "def compute_gradients(images, target_class_idx):\n",
    "  images = tf.convert_to_tensor(images)\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(tf.convert_to_tensor(images))\n",
    "    logits = trained_encoder(tf.convert_to_tensor(images))\n",
    "    probs = logits[:,target_class_idx]#tf.nn.softmax(logits, axis=-1)[:, target_class_idx]\n",
    "  return tape.gradient(probs, images)\n",
    "\n",
    "def integral_approximation(gradients):\n",
    "  # riemann_trapezoidal\n",
    "  grads = (gradients[:-1] + gradients[1:]) / tf.constant(2.0)\n",
    "  integrated_gradients = tf.math.reduce_mean(grads, axis=0)\n",
    "  return integrated_gradients\n",
    "\n",
    "@tf.function\n",
    "def one_batch(baseline, image, alpha_batch, target_class_idx):\n",
    "    # Generate interpolated inputs between baseline and input.\n",
    "    interpolated_path_input_batch = interpolate_images(baseline=baseline,\n",
    "                                                       image=image,\n",
    "                                                       alphas=alpha_batch)\n",
    "\n",
    "    # Compute gradients between model outputs and interpolated inputs.\n",
    "    gradient_batch = compute_gradients(images=interpolated_path_input_batch,\n",
    "                                       target_class_idx=target_class_idx)\n",
    "    return gradient_batch\n",
    "\n",
    "def integrated_gradients(baseline,\n",
    "                         image,\n",
    "                         target_class_idx,\n",
    "                         m_steps=50,\n",
    "                         batch_size=32):\n",
    "  # Generate alphas.\n",
    "  alphas = tf.linspace(start=0.0, stop=1.0, num=m_steps+1)\n",
    "\n",
    "  # Collect gradients.    \n",
    "  gradient_batches = []\n",
    "\n",
    "  # Iterate alphas range and batch computation for speed, memory efficiency, and scaling to larger m_steps.\n",
    "  for alpha in tf.range(0, len(alphas), batch_size):\n",
    "    from_ = alpha\n",
    "    to = tf.minimum(from_ + batch_size, len(alphas))\n",
    "    alpha_batch = alphas[from_:to]\n",
    "    \n",
    "    gradient_batch = one_batch(baseline, image, alpha_batch, target_class_idx)\n",
    "    gradient_batches.append(gradient_batch)\n",
    "\n",
    "  # Concatenate path gradients together row-wise into single tensor.\n",
    "  total_gradients = tf.concat(gradient_batches, axis=0)\n",
    "\n",
    "  # Integral approximation through averaging gradients.\n",
    "  avg_gradients = integral_approximation(gradients=total_gradients)\n",
    "\n",
    "  # Scale integrated gradients with respect to input.\n",
    "  integrated_gradients = (image - baseline) * avg_gradients\n",
    "\n",
    "  return integrated_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca753d0",
   "metadata": {},
   "source": [
    "**Calculating integrated gradients for the first test sample (for demonstration)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff2d7048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function one_batch at 0x153779e9aca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function one_batch at 0x153779e9aca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "int_grad_list = []\n",
    "for i in range(256):\n",
    "    path_gradients = integrated_gradients(baseline=baseline,\n",
    "        image=swe_test_data[0,],\n",
    "        target_class_idx=i,m_steps=m_steps,batch_size=m_steps)\n",
    "    int_grad_list.append(path_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7573ac3",
   "metadata": {},
   "source": [
    "**Save the integrated gradients (optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a13c18e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "os.chdir('./Results_2/')\n",
    "\n",
    "dirOut0 = '../output/python/deephyper/'+dtype+'/'\n",
    "dirOut1 = 'plots_ae_'+dtype+'/N'+str(num_tot)+'/' \n",
    "Current_Model_directory = dirOut0 + dirOut1\n",
    "\n",
    "os.chdir(Current_Model_directory)\n",
    "file_name = 'VIB_int_grad_0.pkl'\n",
    "with open(file_name,'wb') as file:\n",
    "    pickle.dump(int_grad_list,file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe45dd",
   "metadata": {},
   "source": [
    "**Plotting the integrated gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e4e6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def normalize(tensor,min_val,max_val):\n",
    "  \"\"\"Normalizes a tensor between 0 and 1.\n",
    "\n",
    "  Args:\n",
    "    tensor: The tensor to normalize.\n",
    "\n",
    "  Returns:\n",
    "    The normalized tensor.\n",
    "  \"\"\"\n",
    "\n",
    "  return (tensor - min_val) / (max_val - min_val)\n",
    "\n",
    "os.chdir('./Results_2/')\n",
    "dirOut0 = '../output/python/deephyper/'+dtype+'/'\n",
    "dirOut1 = 'plots_ae_'+dtype+'/N'+str(num_tot)+'/' \n",
    "Current_Model_directory = dirOut0 + dirOut1\n",
    "\n",
    "os.chdir(Current_Model_directory)\n",
    "from matplotlib import gridspec\n",
    "nrow = 1\n",
    "ncol = 10\n",
    "fig = plt.figure(figsize=(ncol+1, nrow+1))\n",
    "gs = gridspec.GridSpec(nrow, ncol,\n",
    "                wspace=0, hspace=0, \n",
    "                top=1.-0.4/(nrow+1), bottom=0.0001/(nrow+1), \n",
    "                left=0.5/(ncol+1), right=1-0.5/(ncol+1)) \n",
    "\n",
    "for samp_i,samp_v in enumerate([0]):\n",
    "    file_name = f\"VIB_int_grad_{samp_v}.pkl\"\n",
    "    with open(file_name,'rb') as file:\n",
    "        VIB_int_grad = pickle.load(file)\n",
    "\n",
    "    VIB_hist_data = np.zeros([256*128*128])\n",
    "    mv = 128*128\n",
    "    for i in range(256):\n",
    "        VIB_hist_data[i*mv:(i+1)*mv] = np.abs(tf.reshape(VIB_int_grad[i],[-1]).numpy())\n",
    "\n",
    "    var1 = []\n",
    "    VIB_int_grad_norm = []\n",
    "    for i in range(256):\n",
    "        VIB_int_grad_norm.append(normalize(tf.math.abs(VIB_int_grad[i]),np.min(np.abs(VIB_hist_data)),np.max(np.abs(VIB_hist_data))))\n",
    "        var1.append(tf.math.reduce_sum(tf.cast(VIB_int_grad_norm[i]>0.4,tf.float32)))\n",
    "\n",
    "    tmp = tf.convert_to_tensor(var1)\n",
    "\n",
    "    id_tmp_VIB = tf.argsort(tmp,direction='DESCENDING').numpy()[:10]\n",
    "    \n",
    "    for k, val in enumerate(id_tmp_VIB):\n",
    "                axes = plt.subplot(gs[samp_i,k])\n",
    "                axes.set_xticks([])\n",
    "                axes.set_yticks([])\n",
    "                #axes.axis('off')\n",
    "                tmp = tf.math.abs(VIB_int_grad_norm[val])\n",
    "                pcm = axes.imshow(tmp, vmax=1, vmin=0, cmap='Reds')\n",
    "                axes.set_xlabel(f\"active: {int(var1[val])}\")\n",
    "\n",
    "\n",
    "plt.savefig(\"Pixel_importance_VIB.png\",format = 'png',dpi=500,bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
